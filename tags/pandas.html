<!DOCTYPE html><html><head><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-193041393-1"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-193041393-1', {
              page_path: window.location.pathname,
            });
          </script><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"/><link rel="preload" href="/fonts/FiraCode-VariableFont_wght.ttf" as="font" crossorigin="anonymous"/><link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 100 100&#x27;&gt;&lt;text y=&#x27;.9em&#x27; font-size=&#x27;90&#x27;&gt;üëã&lt;/text&gt;&lt;/svg&gt;"/><meta name="description" content="Bradley Schoeneweis"/><meta property="og:image" content="https://og-image.vercel.app/Bradley%20Schoeneweis.png?theme=light&amp;md=0&amp;fontSize=75px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta name="robots" content="follow, index"/><meta name="og:title" content="Bradley Schoeneweis"/><meta property="og:site_name" content="Bradley Schoeneweis"/><meta property="og:description" content="Bradley Schoeneweis"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@bschoeneweis"/><meta name="twitter:title" content="Bradley Schoeneweis"/><meta name="twitter:description" content="Bradley Schoeneweis"/><meta name="twitter:image" content="https://og-image.vercel.app/Bradley%20Schoeneweis.png?theme=light&amp;md=0&amp;fontSize=75px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><title>Posts tagged &quot;pandas&quot;</title><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/ce6eedf489eb1b9b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ce6eedf489eb1b9b.css" data-n-g=""/><link rel="preload" href="/_next/static/css/186af44e5aae9a7f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/186af44e5aae9a7f.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-e6153bfb4e7e7057.js" defer=""></script><script src="/_next/static/chunks/framework-7d488969745094b0.js" defer=""></script><script src="/_next/static/chunks/main-c3ed9a593ffd1a6c.js" defer=""></script><script src="/_next/static/chunks/pages/_app-059eeeec4cf736c4.js" defer=""></script><script src="/_next/static/chunks/4824-86805e89d9bc00a3.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Btag%5D-8afd6df790ebe176.js" defer=""></script><script src="/_next/static/DPq9pPkbbJxsV6w_am9my/_buildManifest.js" defer=""></script><script src="/_next/static/DPq9pPkbbJxsV6w_am9my/_ssgManifest.js" defer=""></script><script src="/_next/static/DPq9pPkbbJxsV6w_am9my/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="layout_ArticleContainer__QR4qB"><header><div class="layout_LayoutLinks__92rCe"><a href="/">‚Üê Back home</a><a href="/tags">View All Tags</a></div></header><main><header><div class="utils_marginTop2_25__bRt80 utils_marginBottom1_25__XjeT0"><span class="tag_Badge__e6qwu tag_BadgeLarge__O0ytt">pandas</span></div></header><section class="utils_headingMd__gD1Ok utils_padding1rem__xPN3e utils_marginLeft0_5rem__qnumX "><ul class="utils_list__4Mu4l"><li class="utils_listItem__s2m6i"><a href="/posts/forecasting-spy-prices">Forecasting SPY prices using Facebook&#x27;s Prophet</a><br/><div class="utils_marginBottom0_5rem__dsxrK utils_marginTop0_5rem__E7OQF"><div class="tag_TagList__IWJKV"><div class="utils_displayFlex__VCtvW"><a href="/tags/python"><div><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">python</span></div></a></div><div class="utils_displayFlex__VCtvW"><a href="/tags/pandas"><div class="tag_TagListItem__3M0uo"><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">pandas</span></div></a></div><div class="utils_displayFlex__VCtvW"><a href="/tags/data-analysis"><div class="tag_TagListItem__3M0uo"><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">data-analysis</span></div></a></div></div></div><small class="utils_lightText__eUzGY"><time dateTime="2021-05-19">May 19, 2021</time></small></li><li class="utils_listItem__s2m6i"><a href="/posts/visualizing-your-linkedin-connections">Visualizing your LinkedIn Connections using Python</a><br/><div class="utils_marginBottom0_5rem__dsxrK utils_marginTop0_5rem__E7OQF"><div class="tag_TagList__IWJKV"><div class="utils_displayFlex__VCtvW"><a href="/tags/python"><div><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">python</span></div></a></div><div class="utils_displayFlex__VCtvW"><a href="/tags/pandas"><div class="tag_TagListItem__3M0uo"><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">pandas</span></div></a></div><div class="utils_displayFlex__VCtvW"><a href="/tags/data-visualization"><div class="tag_TagListItem__3M0uo"><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">data-visualization</span></div></a></div><div class="utils_displayFlex__VCtvW"><a href="/tags/networkx"><div class="tag_TagListItem__3M0uo"><span class="tag_Badge__e6qwu utils_cursorPointer__oXutf">networkx</span></div></a></div></div></div><small class="utils_lightText__eUzGY"><time dateTime="2021-04-08">April 8, 2021</time></small></li></ul></section></main></div><footer class="footer_FooterContainer__oKXDJ"><div class="footer_Footer__wsKMY"><div class="footer_FooterName__4egKC">¬© <time>2022</time> Bradley Schoeneweis</div><div class="footer_FooterLinks__2mme5"><a href="https://www.linkedin.com/in/bradley-schoeneweis/" target="_blank"><img width="16" src="/icons/linkedin.svg" alt="linkedin logo" class="footer_SocialLinkLogo__OWB_i"/></a><a href="https://github.com/bschoeneweis" target="_blank"><img width="16" src="/icons/github.svg" alt="github logo" class="footer_SocialLinkLogo__OWB_i"/></a><a href="https://medium.com/@bradley-schoeneweis" target="_blank"><img width="16" src="/icons/medium.svg" alt="medium logo" class="footer_SocialLinkLogo__OWB_i"/></a><a href="https://dev.to/bschoeneweis" target="_blank"><img width="16" src="/icons/dev.svg" alt="dev logo" class="footer_SocialLinkLogo__OWB_i"/></a><a href="/feed.xml"><img width="16" src="/icons/rss.svg" alt="rss logo" class="footer_SocialLinkLogo__OWB_i"/></a></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tag":"pandas","taggedPosts":[{"id":"forecasting-spy-prices","contentHtml":"\u003ch2\u003etl;dr\u003c/h2\u003e\n\u003ch3\u003eGoal\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eTo apply Facebook's Prophet forecasting procedure to historical SPY (SPDR S\u0026#x26;P 500 ETF Trust) market data to gather future pricing predictions.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eA few notes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eI'm by no means a data scientist, so this is more of an exploratory analysis than an accurate one\u003c/li\u003e\n\u003cli\u003eFor sake of brevity, I won't be using a training/test split or measuring the error of the model, I will just train the model on the entire dataset and then make a prediction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eProcess overview\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDownloading the data\u003c/strong\u003e - exporting the data from Yahoo Finance as a CSV\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExploring the data\u003c/strong\u003e - loading and exploring the data using Pandas\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFitting the model\u003c/strong\u003e - reading in the data and applying a basic fit of the Prophet model to the data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualizing the forecast\u003c/strong\u003e - visualizing the forecasted pricing data\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003ePython dependencies\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eimport pandas as pd\nfrom prophet import Prophet\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eBefore we jump in, let's give a little background on SPY and on Facebook's Prophet.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eSPDR S\u0026#x26;P 500 ETF Trust\u003c/em\u003e (SPY) is an ETF (\u003cem\u003eExchange Traded Fund\u003c/em\u003e) that tracks the performance of the S\u0026#x26;P 500 index.  SPY is also the largest ETF in the world, and is popular compared to other ETFs that track the S\u0026#x26;P 500 because of the high volume, or the number of shares that trade on a given day (we'll be able to see the volume per day in the CSV we export from Yahoo Finance).\u003c/p\u003e\n\u003cp\u003eFor more information on ETFs, \u003ca href=\"https://www.investopedia.com/terms/e/etf.asp\"\u003eInvestopedia gives a good overview\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://facebook.github.io/prophet/\"\u003eFacebook Prophet\u003c/a\u003e is an open source, automated forecasting procedure for time series data.  I'm not going to dive too much into the mathematics or implementation details of Prophet, but if you are more interested, you can read the \u003ca href=\"https://peerj.com/preprints/3190/\"\u003eresearch paper\u003c/a\u003e.  Prophet makes it easy to handle outliers, adjust to different time intervals, deal with holidays, and leaves the ability to easily tune the forecasting model.\u003c/p\u003e\n\u003cp\u003eNow that we have a general idea of what we're trying to predict and the tool we'll use to forecast, let's dive into the actual data.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eDownloading the data\u003c/h2\u003e\n\u003cp\u003eThanks to Yahoo Finance, we can download historical pricing data for free. You can click \u003ca href=\"https://finance.yahoo.com/quote/SPY/history?p=SPY\"\u003ehere\u003c/a\u003e to view the SPY historical pricing data.\u003c/p\u003e\n\u003cp\u003eClick on the \u003ccode\u003eHistorical Data\u003c/code\u003e tab, and then we can adjust our \u003ccode\u003eTime Period\u003c/code\u003e to the Max as seen below (back to January 1993).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/forecasting-spy/export-data.jpg\" alt=\"Historical pricing data {priority}{680x243}\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we can click download to get our CSV and start diving into the data.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eExploring the data\u003c/h2\u003e\n\u003cp\u003eLet's fire up Pandas and load our data into a DataFrame to see what general insights we can extract.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf = pd.read_csv('SPY.csv')\n\n# Columns and row count\ndf.info()\n\"\"\"\n\u0026#x3C;class 'pandas.core.frame.DataFrame'\u003e\nRangeIndex: 7125 entries, 0 to 7124\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Date       7125 non-null   object \n 1   Open       7125 non-null   float64\n 2   High       7125 non-null   float64\n 3   Low        7125 non-null   float64\n 4   Close      7125 non-null   float64\n 5   Adj Close  7125 non-null   float64\n 6   Volume     7125 non-null   int64  \ndtypes: float64(5), int64(1), object(1)\nmemory usage: 389.8+ KB\n\"\"\"\n\n# Preview of the data\ndf.head()\n\"\"\"\n         Date      Open      High       Low     Close  Adj Close   Volume\n0  1993-01-29  43.96875  43.96875  43.75000  43.93750  25.884184  1003200\n1  1993-02-01  43.96875  44.25000  43.96875  44.25000  26.068277   480500\n2  1993-02-02  44.21875  44.37500  44.12500  44.34375  26.123499   201300\n3  1993-02-03  44.40625  44.84375  44.37500  44.81250  26.399649   529400\n4  1993-02-04  44.96875  45.09375  44.46875  45.00000  26.510111   531500\n\"\"\"\n\n# General statistics\ndf.describe().loc[['mean', 'min', 'max']]\n\"\"\"\n            Open        High         Low       Close   Adj Close        Volume\nmean  146.896395  147.766581  145.928716  146.896373  121.611954  8.453727e+07\nmin    43.343750   43.531250   42.812500   43.406250   25.571209  5.200000e+03\nmax   422.500000  422.820007  419.160004  422.119995  422.119995  8.710263e+08\n\"\"\"\n\n# Day to day percent changes of Highs\ndf[['Date', 'High']].set_index('Date').pct_change().reset_index()\n\"\"\"\n            Date      High\n0     1993-01-29       NaN\n1     1993-02-01  0.006397\n2     1993-02-02  0.002825\n3     1993-02-03  0.010563\n4     1993-02-04  0.005575\n         ...       ...\n7120  2021-05-10 -0.000189\n7121  2021-05-11 -0.017670\n7122  2021-05-12 -0.006454\n7123  2021-05-13 -0.000582\n7124  2021-05-14  0.012465\n\n[7125 rows x 2 columns]\n\"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that we know a bit more about our data in general, we can create a model using Prophet.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eFitting the model\u003c/h2\u003e\n\u003cp\u003eSince we're not concerned in this post about making our model the best it can be, we can train our model on the entire dataset.\u003c/p\u003e\n\u003cp\u003eThis typically isn't a good practice.  When trying to make an accurate prediction, you should use training and test subsets of the data and calculate errors within your model and use those results to tune hyperparameters.\u003c/p\u003e\n\u003cp\u003eNevertheless, let's continue.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# The prophet model fits to a DataFrame with a date column (ds)\n# and a value to predict (y)\ndf_predict = df[['Date', 'Close']]\ndf_predict.columns = ['ds', 'y']\n\n# We can find all of the missing days within our dataset\n# and mark those as \"holidays\"\ndate_series = pd.to_datetime(df['Date'])\ndf_missing_dates = pd\\\n    .date_range(start=date_series.min(), end=date_series.max())\\\n    .difference(date_series)\\\n    .to_frame()\\\n    .reset_index()\ndf_missing_dates.columns = ['holiday', 'ds']\ndf_missing_dates['holiday'] = 'Stock Market Closed'\n\n# Fitting our model is incredibly simple and can be done in the\n# most basic sense in just two lines of code\nm = Prophet(daily_seasonality=True, holidays=df_missing_dates)\nm.fit(df_predict)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eJust like that, we have built our model for a forecast.  All we have left to do is generate dates to predict values for, and run the actual prediction.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eVisualizing the forecast\u003c/h2\u003e\n\u003cp\u003eNow let's forecast with our model and visualize the results.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Create a DataFrame with past and future dates (only weekdays)\nfuture = m.make_future_dataframe(periods=365)\nfuture = future[pd.to_datetime(future['ds']).dt.weekday \u0026#x3C; 5]\n\n# Now we can forecast and visualize in just two more lines of code\nforecast = m.predict(future)\nm.plot(forecast, xlabel='Date', ylabel='Daily Closing Price')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/forecasting-spy/first-prediction.jpg\" alt=\"First SPY forecast {800x480}\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eA few things to notice\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe black dots are the training data points\u003c/li\u003e\n\u003cli\u003eThe blue outline is the confidence interval\u003c/li\u003e\n\u003cli\u003eThe line within the confidence interval is the actual forecast\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBased on our results, we can see the forecast is fairly linear and the confidence interval is relatively narrow (due to the volume of date).  The behavior of the stock market since Covid-19 started back around February 2020 has be a little unorthodox, so let's narrow our model to be trained back to data starting in 2017 to see if there is an effect.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Narrow down to start at 2017\ndf_recent_predict = df_predict.iloc[date_series[date_series.dt.year \u003e 2016].index]\ndate_series = pd.to_datetime(df_recent_predict['ds'])\ndf_recent_missing_dates =  pd\\\n    .date_range(start=date_series.min(), end=date_series.max())\\\n    .difference(date_series)\\\n    .to_frame()\\\n    .reset_index()\ndf_recent_missing_dates.columns = ['holiday', 'ds']\ndf_recent_missing_dates['holiday'] = 'Stock Market Closed'\n\n# Create and fit our new model\nm = Prophet(daily_seasonality=True, holidays=df_recent_missing_dates)\nm.fit(df_recent_predict)\n\n# Recreate our future predictions\nfuture = m.make_future_dataframe(periods=365)\nfuture = future[pd.to_datetime(future['ds']).dt.weekday \u0026#x3C; 5]\n\n# Forecast and visualize\nforecast = m.predict(future)\nm.plot(forecast, xlabel='Date', ylabel='Daily Closing Price')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/images/forecasting-spy/second-prediction.jpg\" alt=\"Second SPY forecast {800x480}\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we can see a much wider confidence interval and a bit more of a bumpy forecast line; however, this looks much more realistic in terms of stock market prediction.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eAll in all, Facebook's Prophet is a very fast, impressive, and strongly abstracted library.  The entire script, including reading in the data, training and forecasting two models, and plotting both of the forecasts took right around \u003cstrong\u003e25 seconds\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eI would love to see this tool in the hands of an actual data scientist to see the accuracy of the models they'd be able to create using Prophet.\u003c/p\u003e\n\u003chr\u003e\n","markdown":"\n## tl;dr\n\n### Goal\n_To apply Facebook's Prophet forecasting procedure to historical SPY (SPDR S\u0026P 500 ETF Trust) market data to gather future pricing predictions._\n\n### A few notes\n- I'm by no means a data scientist, so this is more of an exploratory analysis than an accurate one\n- For sake of brevity, I won't be using a training/test split or measuring the error of the model, I will just train the model on the entire dataset and then make a prediction\n\n### Process overview\n1. **Downloading the data** - exporting the data from Yahoo Finance as a CSV\n2. **Exploring the data** - loading and exploring the data using Pandas\n3. **Fitting the model** - reading in the data and applying a basic fit of the Prophet model to the data\n4. **Visualizing the forecast** - visualizing the forecasted pricing data\n\n### Python dependencies\n```python\nimport pandas as pd\nfrom prophet import Prophet\n```\n\n\u003cp style=\"background-color: orange; padding: 7px 20px; border-radius: 6px;\"\u003e\n    \u003cb\u003eImportant\u003c/b\u003e This article is not investment advice, please conduct your own due diligence. This is merely a simple analysis.\n\u003c/p\u003e\n\n---\n\nBefore we jump in, let's give a little background on SPY and on Facebook's Prophet.\n\nThe _SPDR S\u0026P 500 ETF Trust_ (SPY) is an ETF (_Exchange Traded Fund_) that tracks the performance of the S\u0026P 500 index.  SPY is also the largest ETF in the world, and is popular compared to other ETFs that track the S\u0026P 500 because of the high volume, or the number of shares that trade on a given day (we'll be able to see the volume per day in the CSV we export from Yahoo Finance).\n\nFor more information on ETFs, [Investopedia gives a good overview](https://www.investopedia.com/terms/e/etf.asp).\n\n[Facebook Prophet](https://facebook.github.io/prophet/) is an open source, automated forecasting procedure for time series data.  I'm not going to dive too much into the mathematics or implementation details of Prophet, but if you are more interested, you can read the [research paper](https://peerj.com/preprints/3190/).  Prophet makes it easy to handle outliers, adjust to different time intervals, deal with holidays, and leaves the ability to easily tune the forecasting model.\n\nNow that we have a general idea of what we're trying to predict and the tool we'll use to forecast, let's dive into the actual data.\n\n---\n\n## Downloading the data\nThanks to Yahoo Finance, we can download historical pricing data for free. You can click [here](https://finance.yahoo.com/quote/SPY/history?p=SPY) to view the SPY historical pricing data.\n\nClick on the `Historical Data` tab, and then we can adjust our `Time Period` to the Max as seen below (back to January 1993).\n\n![Historical pricing data {priority}{680x243}](/images/forecasting-spy/export-data.jpg)\n\nNow we can click download to get our CSV and start diving into the data.\n\n---\n\n## Exploring the data\nLet's fire up Pandas and load our data into a DataFrame to see what general insights we can extract.\n\n```python\ndf = pd.read_csv('SPY.csv')\n\n# Columns and row count\ndf.info()\n\"\"\"\n\u003cclass 'pandas.core.frame.DataFrame'\u003e\nRangeIndex: 7125 entries, 0 to 7124\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Date       7125 non-null   object \n 1   Open       7125 non-null   float64\n 2   High       7125 non-null   float64\n 3   Low        7125 non-null   float64\n 4   Close      7125 non-null   float64\n 5   Adj Close  7125 non-null   float64\n 6   Volume     7125 non-null   int64  \ndtypes: float64(5), int64(1), object(1)\nmemory usage: 389.8+ KB\n\"\"\"\n\n# Preview of the data\ndf.head()\n\"\"\"\n         Date      Open      High       Low     Close  Adj Close   Volume\n0  1993-01-29  43.96875  43.96875  43.75000  43.93750  25.884184  1003200\n1  1993-02-01  43.96875  44.25000  43.96875  44.25000  26.068277   480500\n2  1993-02-02  44.21875  44.37500  44.12500  44.34375  26.123499   201300\n3  1993-02-03  44.40625  44.84375  44.37500  44.81250  26.399649   529400\n4  1993-02-04  44.96875  45.09375  44.46875  45.00000  26.510111   531500\n\"\"\"\n\n# General statistics\ndf.describe().loc[['mean', 'min', 'max']]\n\"\"\"\n            Open        High         Low       Close   Adj Close        Volume\nmean  146.896395  147.766581  145.928716  146.896373  121.611954  8.453727e+07\nmin    43.343750   43.531250   42.812500   43.406250   25.571209  5.200000e+03\nmax   422.500000  422.820007  419.160004  422.119995  422.119995  8.710263e+08\n\"\"\"\n\n# Day to day percent changes of Highs\ndf[['Date', 'High']].set_index('Date').pct_change().reset_index()\n\"\"\"\n            Date      High\n0     1993-01-29       NaN\n1     1993-02-01  0.006397\n2     1993-02-02  0.002825\n3     1993-02-03  0.010563\n4     1993-02-04  0.005575\n         ...       ...\n7120  2021-05-10 -0.000189\n7121  2021-05-11 -0.017670\n7122  2021-05-12 -0.006454\n7123  2021-05-13 -0.000582\n7124  2021-05-14  0.012465\n\n[7125 rows x 2 columns]\n\"\"\"\n```\n\nNow that we know a bit more about our data in general, we can create a model using Prophet.\n\n---\n\n## Fitting the model\n\nSince we're not concerned in this post about making our model the best it can be, we can train our model on the entire dataset.\n\nThis typically isn't a good practice.  When trying to make an accurate prediction, you should use training and test subsets of the data and calculate errors within your model and use those results to tune hyperparameters.\n\nNevertheless, let's continue.\n\n```python\n# The prophet model fits to a DataFrame with a date column (ds)\n# and a value to predict (y)\ndf_predict = df[['Date', 'Close']]\ndf_predict.columns = ['ds', 'y']\n\n# We can find all of the missing days within our dataset\n# and mark those as \"holidays\"\ndate_series = pd.to_datetime(df['Date'])\ndf_missing_dates = pd\\\n    .date_range(start=date_series.min(), end=date_series.max())\\\n    .difference(date_series)\\\n    .to_frame()\\\n    .reset_index()\ndf_missing_dates.columns = ['holiday', 'ds']\ndf_missing_dates['holiday'] = 'Stock Market Closed'\n\n# Fitting our model is incredibly simple and can be done in the\n# most basic sense in just two lines of code\nm = Prophet(daily_seasonality=True, holidays=df_missing_dates)\nm.fit(df_predict)\n```\n\nJust like that, we have built our model for a forecast.  All we have left to do is generate dates to predict values for, and run the actual prediction.\n\n---\n\n## Visualizing the forecast\nNow let's forecast with our model and visualize the results.\n\n```python\n# Create a DataFrame with past and future dates (only weekdays)\nfuture = m.make_future_dataframe(periods=365)\nfuture = future[pd.to_datetime(future['ds']).dt.weekday \u003c 5]\n\n# Now we can forecast and visualize in just two more lines of code\nforecast = m.predict(future)\nm.plot(forecast, xlabel='Date', ylabel='Daily Closing Price')\n```\n\n![First SPY forecast {800x480}](/images/forecasting-spy/first-prediction.jpg)\n\n\u003e **A few things to notice**\n\u003e - The black dots are the training data points\n\u003e - The blue outline is the confidence interval\n\u003e - The line within the confidence interval is the actual forecast\n\nBased on our results, we can see the forecast is fairly linear and the confidence interval is relatively narrow (due to the volume of date).  The behavior of the stock market since Covid-19 started back around February 2020 has be a little unorthodox, so let's narrow our model to be trained back to data starting in 2017 to see if there is an effect.\n\n```python\n# Narrow down to start at 2017\ndf_recent_predict = df_predict.iloc[date_series[date_series.dt.year \u003e 2016].index]\ndate_series = pd.to_datetime(df_recent_predict['ds'])\ndf_recent_missing_dates =  pd\\\n    .date_range(start=date_series.min(), end=date_series.max())\\\n    .difference(date_series)\\\n    .to_frame()\\\n    .reset_index()\ndf_recent_missing_dates.columns = ['holiday', 'ds']\ndf_recent_missing_dates['holiday'] = 'Stock Market Closed'\n\n# Create and fit our new model\nm = Prophet(daily_seasonality=True, holidays=df_recent_missing_dates)\nm.fit(df_recent_predict)\n\n# Recreate our future predictions\nfuture = m.make_future_dataframe(periods=365)\nfuture = future[pd.to_datetime(future['ds']).dt.weekday \u003c 5]\n\n# Forecast and visualize\nforecast = m.predict(future)\nm.plot(forecast, xlabel='Date', ylabel='Daily Closing Price')\n```\n\n![Second SPY forecast {800x480}](/images/forecasting-spy/second-prediction.jpg)\n\nNow we can see a much wider confidence interval and a bit more of a bumpy forecast line; however, this looks much more realistic in terms of stock market prediction.\n\n---\n\n## Conclusion\n\nAll in all, Facebook's Prophet is a very fast, impressive, and strongly abstracted library.  The entire script, including reading in the data, training and forecasting two models, and plotting both of the forecasts took right around **25 seconds**.\n\nI would love to see this tool in the hands of an actual data scientist to see the accuracy of the models they'd be able to create using Prophet.\n\n---\n","title":"Forecasting SPY prices using Facebook's Prophet","date":"2021-05-19","tags":["python","pandas","data-analysis"],"description":"Using Facebook‚Äôs Prophet, an open-source, time series forecasting procedure to predict SPY (SPDR S\u0026P 500 ETF Trust) closing prices."},{"id":"visualizing-your-linkedin-connections","contentHtml":"\u003ch2\u003etl;dr\u003c/h2\u003e\n\u003ch3\u003eGoal\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eTo understand and visualize the companies within my directly connected network on LinkedIn\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eProcess overview\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLinkedIn data sources\u003c/strong\u003e - retrieving LinkedIn Network data from a \"Get a copy of your data\" CSV export\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDiving into the data\u003c/strong\u003e - exploring, cleaning, and aggregating the data with \u003ca href=\"https://pandas.pydata.org/\"\u003e\u003ccode\u003ePandas\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCreating the network\u003c/strong\u003e - creating a network graph using \u003ca href=\"https://networkx.org/\"\u003e\u003ccode\u003eNetworkX\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualization\u003c/strong\u003e - visualizing the network with \u003ca href=\"https://pyvis.readthedocs.io/en/latest/\"\u003e\u003ccode\u003epyvis\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImproving the output\u003c/strong\u003e - cleaning up the network graph with additional filtering\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eResults\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eHover over the nodes for more details\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/network/first-nx-graph.html\"\u003eThe first network graph\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/network/second-nx-graph.html\"\u003eThe second (more specific) network graph\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003ePython dependencies\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e# Python standard library\nfrom difflib import get_close_matches\n\n# 3rd party\nimport networkx as nx\nimport pandas as pd\nfrom pyvis.network import Network\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eRecently, I was exploring \u003ca href=\"https://www.linkedin.com/in/bradley-schoeneweis/\"\u003emy LinkedIn\u003c/a\u003e network to see what some of my colleagues from high school and undergrad are currently up to.\u003c/p\u003e\n\u003cp\u003eAs I was scrolling through the connections page, I noticed LinkedIn gives you options to filter and searching with ease, but it doesn't really provide tools to learn about your network as a whole.\u003c/p\u003e\n\u003cp\u003eSo I decided to see if there was an easy way to export my network data to see what I could do with a few hours of exploring the data.\u003c/p\u003e\n\u003ch2\u003eLinkedIn data sources\u003c/h2\u003e\n\u003cp\u003eMy first thought was to checkout out the \u003ca href=\"https://www.linkedin.com/developers/\"\u003eLinkedIn's Developer API\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSomething I do fairly frequently at my current job is integrating various 3rd-party REST APIs into our platform, so I wanted to see all the functionality and possibilities that this API would provide.\u003c/p\u003e\n\u003cp\u003eAfter reading through some documentation, I decided this wasn't a direction I wanted to pursue. Most of their developer products require approval, so I decided to look into other options.\u003c/p\u003e\n\u003cp\u003eAnother thought I had was to write a quick scraping script to pull down the HTML of my connections page and parse out names and companies, but I assumed there had to be a more simple way to get this data.\u003c/p\u003e\n\u003cp\u003eFinally, after a bit of research, I found that there are various \"Get a copy of your data\" reports that you can run within LinkedIn.  In order to get to these reports, you can do the following:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOn the homepage toolbar, click the \u003cstrong\u003eMe\u003c/strong\u003e dropdown\u003c/li\u003e\n\u003cli\u003eUnder the \u003cem\u003eAccount\u003c/em\u003e section, click \u003cstrong\u003eSettings \u0026#x26; Privacy\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eClick on \u003cstrong\u003eGet a copy of your data\u003c/strong\u003e, and you can view the various reports\u003c/li\u003e\n\u003cli\u003eSelect the reports you're interested in, for this, I just checked \u003cstrong\u003eConnections\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAfter requesting the report, it should only take a few minutes before you get an email saying your report is ready for export.\u003c/p\u003e\n\u003ch2\u003eDiving into the data\u003c/h2\u003e\n\u003cp\u003eTo reiterate our goal, we want to get a broad understanding of the companies within the first layer of our network (direct connections). Now, let's load up Python and learn more about this data in this CSV.\u003c/p\u003e\n\u003ch3\u003eReading in the data\u003c/h3\u003e\n\u003cp\u003eOnce the CSV is downloaded, we can open it up with Pandas and take a look (\u003cem\u003eoutput will be commented below\u003c/em\u003e).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport pandas as pd\n\n# We want to skip the first three rows because of Notes at the top\ndf = pd.read_csv('Connections.csv', skiprows=3)\n\ndf.columns\n# ['First Name', 'Last Name', 'Email Address', 'Company', 'Position', 'Connected On',]\n\ndf.info()\n\"\"\"\n\u0026#x3C;class 'pandas.core.frame.DataFrame'\u003e\nRangeIndex: 376 entries, 0 to 375\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   First Name     375 non-null    object\n 1   Last Name      375 non-null    object\n 2   Email Address  1 non-null      object\n 3   Company        371 non-null    object\n 4   Position       371 non-null    object\n 5   Connected On   376 non-null    object\ndtypes: object(6)\nmemory usage: 17.8+ KB\n\"\"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI won't post the name's of any individuals or full rows to respect the privacy of my connections, but when I searched through the my Connections CSV, I noticed a few initial patterns that would help clean up the data.\u003c/p\u003e\n\u003ch3\u003eCleaning up the data\u003c/h3\u003e\n\u003cp\u003eAt first glance, the first thing I notice is connections who don't list a current company, so let's get rid of those.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf = df[df['Company'].notna()].sort_values(by='Company')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter sorting, another thing I noticed was that some of these company names belong to the same company, but the individuals wrote them differently.\u003c/p\u003e\n\u003cp\u003eAn example of this is \u003ccode\u003e'IBM Global Solution Center'\u003c/code\u003e and \u003ccode\u003e'IBM'\u003c/code\u003e; for our purposes, these should both be classified as \u003ccode\u003eIBM\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eLet's run through a fuzzy match run using \u003ca href=\"https://docs.python.org/3/library/difflib.html#difflib.get_close_matches\"\u003edifflib's \u003ccode\u003eget_close_matches\u003c/code\u003e\u003c/a\u003e to try and bucket some of these similar company names.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom difflib import get_close_matches\n\ncompanies = df['Company'].drop_duplicates()\n\n# cutoff=0.7 is a similarity ranking, and n=10 just takes the top 10 values\nsimilar_companies = {x: get_close_matches(x, companies, n=10, cutoff=0.7)\n                     for x in companies}\n\n# We are only interested in the entries that had another match\nsimilar_companies = {x: [name for name in y if name != x]\n                     for x, y in similar_companies.items() if len(y) \u003e 1}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, this solution is not perfect, but it will help draw out some similar companies. You should still run a manual inspection of the data (the IBM example I gave above is one that doesn't show up in the fuzzy match results).\u003c/p\u003e\n\u003cp\u003eBased upon the results, let's group together some of the companies that had matches.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf['Company'] = df['Company'].replace({\n    'KPMG US': similar_companies['KPMG US'],\n    'Self-employed': similar_companies['Self-employed'],\n    'IBM Global Solution Center': 'IBM',\n})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe next thing you may have noticed is that in our \u003ccode\u003esimilar_companies\u003c/code\u003e dictionary, we cleaned up a \u003ccode\u003eSelf-employed\u003c/code\u003e entry.\u003c/p\u003e\n\u003cp\u003eTo stay aligned with our goal, let's drop these entries, as well as your current company.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecompanies_to_drop = ['self employed', 'your current company']\ndf = df[~df['Company'].str.lower().isin(companies_to_drop)]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eAggregating the data\u003c/h3\u003e\n\u003cp\u003eNow that our data is cleaned up a bit, let's aggregate and sum the number of connections for each of the companies.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf_company_counts = df['Company'].value_counts().reset_index()\ndf_company_counts.columns = ['Company', 'Count']  # For ease of understanding\ndf_company_counts = df_company_counts.sort_values(by='Count', ascending=False)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eCreating the network\u003c/h2\u003e\n\u003cp\u003eWe have the numbers we want for each company, now let's jump into using \u003ccode\u003eNetworkX\u003c/code\u003e to recreate a network.\u003c/p\u003e\n\u003cp\u003eThe first step will be to initialize our graph, and add yourself as the central node, as it is your network.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport networkx as nx\n\nG = nx.Graph()\nG.add_node('Me')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, we'll loop through our \u003ccode\u003edf_company_counts\u003c/code\u003e DataFrame and add each company as a node.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eYou'll notice some HTML tags in the title below, this is just to make it more readable for later\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efor _, row in df_company_counts.iterrows():\n\t# The title will be for more information later on\n    title = '\u0026#x3C;b\u003e{0}\u0026#x3C;/b\u003e ({1})\u0026#x3C;br\u003e\u0026#x3C;hr\u003ePositions:\u0026#x3C;br\u003e'.format(row['Company'],\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\trow['Count'])\n\n    # In addition to the full company name, let's add each position in a\n    # list to see the roles our connections have at these companies\n    position_list = ''.join('\u0026#x3C;li\u003e{}\u0026#x3C;/li\u003e'.format(x)\n    \t\t\t\t\t\tfor x in df[df['Company'] == row['Company']]['Position'])\n    title += '\u0026#x3C;ul\u003e{0}\u0026#x3C;/ul\u003e'.format(position_list)\n\n    # For ease of viewing, limit company names to 15 letters\n    node_name = row['Company']\n    if len(node_name) \u003e 15:\n        node_name = node_name[:15] + '...'\n\n    # Add the node and an edge connection ourself to the new node\n    G.add_node(node_name, weight=row['Count'], size=row['Count'] * 2, title=title)\n    G.add_edge('Me', node_name)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd just like that, we've created our network of connections.\u003c/p\u003e\n\u003ch2\u003eVisualization\u003c/h2\u003e\n\u003cp\u003eOur network graph is created, so let's get into visualizing the network.\u003c/p\u003e\n\u003cp\u003eThere are a few options for visualizing networks including \u003ccode\u003ematplotlib.pyplot\u003c/code\u003e, but I found that \u003ccode\u003epyvis\u003c/code\u003e was the easiest to use for several reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epyvis\u003c/code\u003e generates an HTML file\u003c/li\u003e\n\u003cli\u003eCustomization is made very easy\u003c/li\u003e\n\u003cli\u003eThe graph is interactive by default\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet's look into generating this HTML file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom pyvis.network import Network\n\nnt = Network('100%', '100%', bgcolor='#222222', font_color='white')\nnt.from_nx(G)\nnt.repulsion()  # Spaces out the nodes\nnt.show('nx.html')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd it's that simple! We specify a width and height, optional styling attributes, and then we can generate the network graph visual straight from what we created with NetworkX.\u003c/p\u003e\n\u003cp\u003eNow we can see \u003ca href=\"/network/first-nx-graph.html\"\u003ethe network we generated\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou can hover over each node to see the total number of connections that work at the respective company, and below is a list of the positions held by your connections.\u003c/p\u003e\n\u003cp\u003eAs you can see, this is a bit hard to read into since there are a lot of nodes. Try and imagine reading this with +1,000 connections.\u003c/p\u003e\n\u003ch2\u003eImproving the output\u003c/h2\u003e\n\u003cp\u003eThere are a few ways that our network could be narrowed down.\u003c/p\u003e\n\u003cp\u003eBeing a \u003cem\u003eSoftware Developer\u003c/em\u003e, the thought that first occurred to me was to try and dial in on tech-related companies through known positions titles.\u003c/p\u003e\n\u003cp\u003eTo do this, I thought of a list of buzzwords/common job titles that I've seen across LinkedIn, and filtered down the initial DataFrame.\u003c/p\u003e\n\u003cp\u003eThen, we go through the same process we did in previous sections of generating and displaying the graph.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eAgain, this is not perfect, but it's a good starting point.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Filter down from a list of popular tech positions\npositions = [\n    'developer', 'engineer', 'ai', 'analytics', 'software', 'cloud', 'cto',\n    'sde', 'sre', 'saas', 'product', 'engineering', 'scientist', 'data',\n]\ndf = df[df['Position'].str.contains('|'.join(positions), case=False)]\ndf_company_counts = df['Company'].value_counts().reset_index()\ndf_company_counts.columns = ['Company', 'Count']\ndf_company_counts = df_company_counts.sort_values(by='Count', ascending=False)\n\n# Re-initialize the graph and add the nodes/edges again\nG = nx.Graph()\nG.add_node('Me')\n\nfor _, row in df_company_counts.iterrows():\n    title = '\u0026#x3C;b\u003e{0}\u0026#x3C;/b\u003e ({1})\u0026#x3C;br\u003e\u0026#x3C;hr\u003ePositions:\u0026#x3C;br\u003e'.format(row['Company'], row['Count'])\n    position_list = ''.join('\u0026#x3C;li\u003e{}\u0026#x3C;/li\u003e'.format(x)\n    \t\t\t\t\t\tfor x in df[df['Company'] == row['Company']]['Position'])\n    title += '\u0026#x3C;ul\u003e{0}\u0026#x3C;/ul\u003e'.format(position_list)\n    node_name = row['Company']\n    if len(node_name) \u003e 15:\n        node_name = node_name[:15] + '...'\n\n    # Since there are less nodes, let's increase the sizes\n    G.add_node(node_name, weight=row['Count'], size=row['Count'] * 5, title=title)\n    G.add_edge('Me', node_name)\n\n# Generate the visualization\nnt = Network('100%', '100%', bgcolor='#222222', font_color='white')\nnt.from_nx(G)\nnt.repulsion()\nnt.show('nx.html')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, let's look at the \u003ca href=\"/network/second-nx-graph.html\"\u003eupdated results\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMuch better! This is more readable and easier to interact with.\u003c/p\u003e\n\u003cp\u003eAnd just like that, we achieved our goal of gaining a broader understanding of the companies in our LinkedIn network.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePossible improvements for those interested\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eScraping the profile location of each of your connections to segment by location\u003c/li\u003e\n\u003cli\u003eCompiling a list of companies you'd like to work for/are interested in and creating a filtering system\u003c/li\u003e\n\u003cli\u003eResearching salary data for positions and gathering average pay by company\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n","markdown":"\n## tl;dr\n\n### Goal\n_To understand and visualize the companies within my directly connected network on LinkedIn_\n\n### Process overview\n1. **LinkedIn data sources** - retrieving LinkedIn Network data from a \"Get a copy of your data\" CSV export\n2. **Diving into the data** - exploring, cleaning, and aggregating the data with [`Pandas`](https://pandas.pydata.org/)\n3. **Creating the network** - creating a network graph using [`NetworkX`](https://networkx.org/)\n4. **Visualization** - visualizing the network with [`pyvis`](https://pyvis.readthedocs.io/en/latest/)\n5. **Improving the output** - cleaning up the network graph with additional filtering\n\n### Results\n_Hover over the nodes for more details_\n- [The first network graph](/network/first-nx-graph.html)\n- [The second (more specific) network graph](/network/second-nx-graph.html)\n\n### Python dependencies\n```python\n# Python standard library\nfrom difflib import get_close_matches\n\n# 3rd party\nimport networkx as nx\nimport pandas as pd\nfrom pyvis.network import Network\n```\n\n---\n\nRecently, I was exploring [my LinkedIn](https://www.linkedin.com/in/bradley-schoeneweis/) network to see what some of my colleagues from high school and undergrad are currently up to.\n\nAs I was scrolling through the connections page, I noticed LinkedIn gives you options to filter and searching with ease, but it doesn't really provide tools to learn about your network as a whole.\n\nSo I decided to see if there was an easy way to export my network data to see what I could do with a few hours of exploring the data.\n\n\n## LinkedIn data sources\n\nMy first thought was to checkout out the [LinkedIn's Developer API](https://www.linkedin.com/developers/).\n\nSomething I do fairly frequently at my current job is integrating various 3rd-party REST APIs into our platform, so I wanted to see all the functionality and possibilities that this API would provide.\n\nAfter reading through some documentation, I decided this wasn't a direction I wanted to pursue. Most of their developer products require approval, so I decided to look into other options.\n\nAnother thought I had was to write a quick scraping script to pull down the HTML of my connections page and parse out names and companies, but I assumed there had to be a more simple way to get this data.\n\nFinally, after a bit of research, I found that there are various \"Get a copy of your data\" reports that you can run within LinkedIn.  In order to get to these reports, you can do the following:\n1. On the homepage toolbar, click the **Me** dropdown\n2. Under the _Account_ section, click **Settings \u0026 Privacy**\n3. Click on **Get a copy of your data**, and you can view the various reports\n4. Select the reports you're interested in, for this, I just checked **Connections**\n\nAfter requesting the report, it should only take a few minutes before you get an email saying your report is ready for export.\n\n\n## Diving into the data\n\nTo reiterate our goal, we want to get a broad understanding of the companies within the first layer of our network (direct connections). Now, let's load up Python and learn more about this data in this CSV.\n\n### Reading in the data\nOnce the CSV is downloaded, we can open it up with Pandas and take a look (_output will be commented below_).\n\n```python\nimport pandas as pd\n\n# We want to skip the first three rows because of Notes at the top\ndf = pd.read_csv('Connections.csv', skiprows=3)\n\ndf.columns\n# ['First Name', 'Last Name', 'Email Address', 'Company', 'Position', 'Connected On',]\n\ndf.info()\n\"\"\"\n\u003cclass 'pandas.core.frame.DataFrame'\u003e\nRangeIndex: 376 entries, 0 to 375\nData columns (total 6 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   First Name     375 non-null    object\n 1   Last Name      375 non-null    object\n 2   Email Address  1 non-null      object\n 3   Company        371 non-null    object\n 4   Position       371 non-null    object\n 5   Connected On   376 non-null    object\ndtypes: object(6)\nmemory usage: 17.8+ KB\n\"\"\"\n```\n\nI won't post the name's of any individuals or full rows to respect the privacy of my connections, but when I searched through the my Connections CSV, I noticed a few initial patterns that would help clean up the data.\n\n### Cleaning up the data\n\nAt first glance, the first thing I notice is connections who don't list a current company, so let's get rid of those.\n\n```python\ndf = df[df['Company'].notna()].sort_values(by='Company')\n```\n\nAfter sorting, another thing I noticed was that some of these company names belong to the same company, but the individuals wrote them differently.\n\nAn example of this is `'IBM Global Solution Center'` and `'IBM'`; for our purposes, these should both be classified as `IBM`.\n\nLet's run through a fuzzy match run using [difflib's `get_close_matches`](https://docs.python.org/3/library/difflib.html#difflib.get_close_matches) to try and bucket some of these similar company names.\n```python\nfrom difflib import get_close_matches\n\ncompanies = df['Company'].drop_duplicates()\n\n# cutoff=0.7 is a similarity ranking, and n=10 just takes the top 10 values\nsimilar_companies = {x: get_close_matches(x, companies, n=10, cutoff=0.7)\n                     for x in companies}\n\n# We are only interested in the entries that had another match\nsimilar_companies = {x: [name for name in y if name != x]\n                     for x, y in similar_companies.items() if len(y) \u003e 1}\n```\n\nNow, this solution is not perfect, but it will help draw out some similar companies. You should still run a manual inspection of the data (the IBM example I gave above is one that doesn't show up in the fuzzy match results).\n\nBased upon the results, let's group together some of the companies that had matches.\n```python\ndf['Company'] = df['Company'].replace({\n    'KPMG US': similar_companies['KPMG US'],\n    'Self-employed': similar_companies['Self-employed'],\n    'IBM Global Solution Center': 'IBM',\n})\n```\n\nThe next thing you may have noticed is that in our `similar_companies` dictionary, we cleaned up a `Self-employed` entry.\n\nTo stay aligned with our goal, let's drop these entries, as well as your current company.\n```python\ncompanies_to_drop = ['self employed', 'your current company']\ndf = df[~df['Company'].str.lower().isin(companies_to_drop)]\n```\n\n### Aggregating the data\nNow that our data is cleaned up a bit, let's aggregate and sum the number of connections for each of the companies.\n\n```python\ndf_company_counts = df['Company'].value_counts().reset_index()\ndf_company_counts.columns = ['Company', 'Count']  # For ease of understanding\ndf_company_counts = df_company_counts.sort_values(by='Count', ascending=False)\n```\n\n## Creating the network\n\nWe have the numbers we want for each company, now let's jump into using `NetworkX` to recreate a network.\n\nThe first step will be to initialize our graph, and add yourself as the central node, as it is your network.\n\n```python\nimport networkx as nx\n\nG = nx.Graph()\nG.add_node('Me')\n```\n\nThen, we'll loop through our `df_company_counts` DataFrame and add each company as a node.\n\n_You'll notice some HTML tags in the title below, this is just to make it more readable for later_\n```python\nfor _, row in df_company_counts.iterrows():\n\t# The title will be for more information later on\n    title = '\u003cb\u003e{0}\u003c/b\u003e ({1})\u003cbr\u003e\u003chr\u003ePositions:\u003cbr\u003e'.format(row['Company'],\n    \t\t\t\t\t\t\t\t\t\t\t\t\t\trow['Count'])\n\n    # In addition to the full company name, let's add each position in a\n    # list to see the roles our connections have at these companies\n    position_list = ''.join('\u003cli\u003e{}\u003c/li\u003e'.format(x)\n    \t\t\t\t\t\tfor x in df[df['Company'] == row['Company']]['Position'])\n    title += '\u003cul\u003e{0}\u003c/ul\u003e'.format(position_list)\n\n    # For ease of viewing, limit company names to 15 letters\n    node_name = row['Company']\n    if len(node_name) \u003e 15:\n        node_name = node_name[:15] + '...'\n\n    # Add the node and an edge connection ourself to the new node\n    G.add_node(node_name, weight=row['Count'], size=row['Count'] * 2, title=title)\n    G.add_edge('Me', node_name)\n```\n\nAnd just like that, we've created our network of connections.\n\n\n## Visualization\n\nOur network graph is created, so let's get into visualizing the network.\n\nThere are a few options for visualizing networks including `matplotlib.pyplot`, but I found that `pyvis` was the easiest to use for several reasons:\n- `pyvis` generates an HTML file\n- Customization is made very easy\n- The graph is interactive by default\n\nLet's look into generating this HTML file.\n```python\nfrom pyvis.network import Network\n\nnt = Network('100%', '100%', bgcolor='#222222', font_color='white')\nnt.from_nx(G)\nnt.repulsion()  # Spaces out the nodes\nnt.show('nx.html')\n```\n\nAnd it's that simple! We specify a width and height, optional styling attributes, and then we can generate the network graph visual straight from what we created with NetworkX.\n\nNow we can see [the network we generated](/network/first-nx-graph.html).\n\nYou can hover over each node to see the total number of connections that work at the respective company, and below is a list of the positions held by your connections.\n\nAs you can see, this is a bit hard to read into since there are a lot of nodes. Try and imagine reading this with +1,000 connections.\n\n## Improving the output\n\nThere are a few ways that our network could be narrowed down.\n\nBeing a _Software Developer_, the thought that first occurred to me was to try and dial in on tech-related companies through known positions titles.\n\nTo do this, I thought of a list of buzzwords/common job titles that I've seen across LinkedIn, and filtered down the initial DataFrame.\n\nThen, we go through the same process we did in previous sections of generating and displaying the graph.\n\n_Again, this is not perfect, but it's a good starting point._\n```python\n# Filter down from a list of popular tech positions\npositions = [\n    'developer', 'engineer', 'ai', 'analytics', 'software', 'cloud', 'cto',\n    'sde', 'sre', 'saas', 'product', 'engineering', 'scientist', 'data',\n]\ndf = df[df['Position'].str.contains('|'.join(positions), case=False)]\ndf_company_counts = df['Company'].value_counts().reset_index()\ndf_company_counts.columns = ['Company', 'Count']\ndf_company_counts = df_company_counts.sort_values(by='Count', ascending=False)\n\n# Re-initialize the graph and add the nodes/edges again\nG = nx.Graph()\nG.add_node('Me')\n\nfor _, row in df_company_counts.iterrows():\n    title = '\u003cb\u003e{0}\u003c/b\u003e ({1})\u003cbr\u003e\u003chr\u003ePositions:\u003cbr\u003e'.format(row['Company'], row['Count'])\n    position_list = ''.join('\u003cli\u003e{}\u003c/li\u003e'.format(x)\n    \t\t\t\t\t\tfor x in df[df['Company'] == row['Company']]['Position'])\n    title += '\u003cul\u003e{0}\u003c/ul\u003e'.format(position_list)\n    node_name = row['Company']\n    if len(node_name) \u003e 15:\n        node_name = node_name[:15] + '...'\n\n    # Since there are less nodes, let's increase the sizes\n    G.add_node(node_name, weight=row['Count'], size=row['Count'] * 5, title=title)\n    G.add_edge('Me', node_name)\n\n# Generate the visualization\nnt = Network('100%', '100%', bgcolor='#222222', font_color='white')\nnt.from_nx(G)\nnt.repulsion()\nnt.show('nx.html')\n```\n\nNow, let's look at the [updated results](/network/second-nx-graph.html).\n\nMuch better! This is more readable and easier to interact with.\n\nAnd just like that, we achieved our goal of gaining a broader understanding of the companies in our LinkedIn network.\n\n---\n\n**_Possible improvements for those interested_**\n- Scraping the profile location of each of your connections to segment by location\n- Compiling a list of companies you'd like to work for/are interested in and creating a filtering system\n- Researching salary data for positions and gathering average pay by company\n\n---\n","title":"Visualizing your LinkedIn Connections using Python","date":"2021-04-08","tags":["python","pandas","data-visualization","networkx"],"description":"Using Python's Pandas, NetworkX, and pyvis to understand and visualize companies within a directly connected LinkedIn network."}]},"__N_SSG":true},"page":"/tags/[tag]","query":{"tag":"pandas"},"buildId":"DPq9pPkbbJxsV6w_am9my","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>